{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b46d467-74a8-43e9-ada2-02c3b3cd22ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a15343-b85d-4def-8e1b-a1d911d72131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words(dataset=3):\n",
    "    datasets = ['../datasets/names.txt',\n",
    "                '../datasets/male_names_rus.txt',\n",
    "                '../datasets/female_names_rus.txt',\n",
    "                '../datasets/russian_surnames.txt',\n",
    "                '../datasets/towns.txt']\n",
    "\n",
    "    filename = datasets[dataset]\n",
    "\n",
    "    if filename is None:\n",
    "        print(f'Dataset \"{dataset}\" not found')\n",
    "        return []\n",
    "    \n",
    "    data = open(filename, 'r').read().lower()\n",
    "\n",
    "    if dataset == 3:\n",
    "        data = data\\\n",
    "            .replace('a', 'а')\\\n",
    "            .replace('c', 'с')\\\n",
    "            .replace('k', 'к')\\\n",
    "            .replace('o', 'о')\\\n",
    "            .replace('\\x1b', '')\\\n",
    "            .replace('1', '')\n",
    "    \n",
    "    return [w.strip() for w in data.splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ce13fb7-f193-435d-845a-614712082c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(words)=251337\n",
      "words[:10]=['аалферова', 'ааль', 'ааман', 'аамана', 'ааманая', 'ааманий', 'аандреева', 'аарон', 'ааронова', 'аб']\n"
     ]
    }
   ],
   "source": [
    "words = get_words(3)\n",
    "print(f'{len(words)=}')\n",
    "print(f'{words[:10]=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bdf1914-9b8b-4abc-8007-3328426a1f30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The most intuitive approach to the Make More problem is to count how many times each character\n",
    "#   appears on the second place in all pairs of characters (bigrams) we can find in the dataset\n",
    "# In this example our goal is to combine characters into words which means we should predict the next character\n",
    "#   potentially given some input characters\n",
    "# It is a classification task with len(characters) + 1 classes (here and below we use len(characters) + 1 = 27 for brevity)\n",
    "#   and the output is a vector op probabilites\n",
    "# This also implies awareness of the start and the end of each word which we achive by adding a special character 0\n",
    "#   before and after each word\n",
    "\n",
    "class Counter():\n",
    "    def __init__(self, words):\n",
    "        self.chars = ['.'] + sorted(list(set(''.join(words)))) # '.' is going to be used to indicate start and end of words\n",
    "\n",
    "        self.stoi = {s: i for i, s in enumerate(self.chars)}\n",
    "        self.itos = {i: s for s, i in self.stoi.items()}\n",
    "\n",
    "        print(f'total words {len(words)}, total chars {len(self.chars)}, chars{self.chars}')\n",
    "        \n",
    "        # Count all possible bigrams\n",
    "        # count = {}\n",
    "        # for w in words:\n",
    "        #     word = '.' + w + '.'\n",
    "        #     for c1, c2 in zip(word, word[1:]):\n",
    "        #         bigram = (c1, c2)\n",
    "        #         count[bigram] = count.get(bigram, 0) + 1\n",
    "        # sorted(count.items(), key=lambda kv: -kv[1])[:20]\n",
    "\n",
    "        self.N = torch.zeros((len(self.chars), len(self.chars)), dtype=torch.int32)\n",
    "        self.P = None\n",
    "        self.g_prob = None\n",
    "        self.g = None\n",
    "\n",
    "        # Bigram occurances Tensor\n",
    "        #   where rows represent the first character index and columns the last one\n",
    "        for w in tqdm(words, 'Processing words'):\n",
    "            word = '.' + w + '.'\n",
    "            for c1, c2 in zip(word, word[1:]):\n",
    "                x = self.stoi[c1]\n",
    "                y = self.stoi[c2]\n",
    "                self.N[x, y] += 1\n",
    "\n",
    "    def plot_char_map(self):\n",
    "        # plt.imshow(N)\n",
    "\n",
    "        plt.figure(figsize=(16, 16))\n",
    "        plt.imshow(self.N, cmap='Blues')\n",
    "\n",
    "        for i in range(len(self.chars)):\n",
    "            for j in range(len(self.chars)):\n",
    "                ch_str = self.itos[i] + self.itos[j]\n",
    "                plt.text(j, i, ch_str, ha='center', va='bottom', color='gray')\n",
    "                plt.text(j, i, self.N[i, j].item(), ha='center', va='top', color='gray')\n",
    "\n",
    "        plt.axis('off')\n",
    "        \n",
    "    def prob(self, x):\n",
    "        # The output is a probability distribution vector (sums to 1)\n",
    "        # We can generate one by deviding each value in a row by the sum of all counts in the row\n",
    "        # Each row represent all 27 possible labels y for a given input x in bigram (x, y)\n",
    "        \n",
    "        # For a single row\n",
    "        p = self.N[x].float()\n",
    "        return p / p.sum()\n",
    "    \n",
    "    def make_more_prob(self, x=10):\n",
    "        # Now we can generate random names based on the probability of occurance of each pair of characters\n",
    "        # To do so we take an input char and choose a random bigram where it appears first with respect\n",
    "        #   to the corresponding probability distribution vector stored in N\n",
    "        # \n",
    "        # See more here at https://pytorch.org/docs/stable/generated/torch.multinomial.html\n",
    "\n",
    "        if self.g_prob is None:\n",
    "            self.g_prob = torch.Generator().manual_seed(2147483647)\n",
    "        \n",
    "        for i in range(x):\n",
    "            name = []\n",
    "            ix = 0 # 0 -> '.' the start of the word\n",
    "            while True:\n",
    "                p = self.prob(ix)\n",
    "                ix = torch.multinomial(p, num_samples=1, replacement=True, generator=self.g_prob).item()\n",
    "                name.append(self.itos[ix])\n",
    "                if ix == 0: # 0 -> '.' the end of the word\n",
    "                    break\n",
    "\n",
    "            print(''.join(name))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def forward(self):\n",
    "        # The same thing as in prob() but converting the entire tensor into a probability one\n",
    "\n",
    "        P = (self.N + 1).float() # Adding +1 for \"smoothing\" so no pairs get 0 probability for us to never fall into log(0) case (see below)\n",
    "\n",
    "        # Using `keepdim` is crucial here (see the docs here: https://pytorch.org/docs/stable/generated/torch.sum.html)\n",
    "        # We need to sum up the values in each row, which means the shape of the result should be (27, 1) - 27 rows 1 value each\n",
    "        # If we don't keep the dimentions torch will discard this dimension and make it (27) instead of (27, 1).\n",
    "        # So when it comes to tensor division according to the division rules (27, 27)/(27) is treated like (27, 27)/(1, 27)\n",
    "        #   which will produce a completely different result from (27, 27)/(27, 1) as it should be\n",
    "        self.P = P / P.sum(1, keepdim=True)\n",
    "\n",
    "        # print(f'{self.P[0].sum().item()=}') # should be 1, as it is a sum of probabilities of all pairs starting with char itos[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def make_more(self, x=10):\n",
    "        if self.g is None:\n",
    "            self.g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "        self.forward()\n",
    "\n",
    "        for i in range(x):\n",
    "            name = []\n",
    "            ix = 0\n",
    "            while True:\n",
    "                p = self.P[ix]\n",
    "                ix = torch.multinomial(p, num_samples=1, replacement=True, generator=self.g).item()\n",
    "                name.append(self.itos[ix])\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(name))\n",
    "            \n",
    "        return self\n",
    "\n",
    "    # here we use words to derive labels and estimate the loss using it\n",
    "    def loss(self, words):\n",
    "        # Now as we've learned pair probabilities from the initial dataset of words\n",
    "        #   we'd like to figure out the quality of our model,\n",
    "        #   i.e. how good our probabilities (aka weights of the model) are\n",
    "        # So the loss function we need to use is a product of all probability distribution vectors\n",
    "        #   of all character pairs aka likelihood\n",
    "        # For convenience we can take log() of it to leverage the property of log(a*b) = log(a) + log(b)\n",
    "        # We then also negate the result to match the semantics of \"minimization of the loss function\",\n",
    "        #   and also consider the average value as the one we eventually need to minimize the closer to 0 the better\n",
    "\n",
    "        log_likelihood = 0.0\n",
    "        pairs_count = 0\n",
    "\n",
    "        # for w in ['denis']:\n",
    "        for w in tqdm(words, 'Processing words'):\n",
    "            chs = '.' + w + '.'\n",
    "            for ch1, ch2 in zip(chs, chs[1:]):\n",
    "                ix1 = self.stoi[ch1]\n",
    "                ix2 = self.stoi[ch2]\n",
    "                p = self.P[ix1, ix2]\n",
    "                likelihood = torch.log(p)\n",
    "                log_likelihood += likelihood\n",
    "                pairs_count += 1\n",
    "                # print(f'{ch1}{ch2} {p:.4f} {likelihood:.4f}')\n",
    "\n",
    "        nll = -log_likelihood\n",
    "        anll = nll / pairs_count\n",
    "\n",
    "        print(f'                 log likelihood: {log_likelihood}')\n",
    "        print(f'        negative log likelihood: {nll}')\n",
    "        print(f'average negative log likelihood: {anll}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa9e8024-65a5-470b-8ccb-30959783b34d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words 1117, total chars 35, chars['.', ' ', '-', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ddf108a0174ade8bd3bbcb0d065f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing words:   0%|          | 0/1117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "counter = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe2532c-4eca-4509-8375-46cef8c2dd60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к.\n",
      "мибубаниноле.\n",
      "новоляноволо.\n",
      "бежейск.\n",
      "амородов.\n",
      "тай лыбав.\n",
      "к.\n",
      "пелещарбибаник.\n",
      "жнь.\n",
      "юразновогускивагурстовецымсск.\n",
      "==================================================\n",
      "к.\n",
      "мибубаниноле.\n",
      "новоляновья.\n",
      "грежейск.\n",
      "амэродов.\n",
      "тай лыбачель-ёурариб-баник.\n",
      "жнь.\n",
      "юразновогу пскагурсткирсг.\n",
      "бск.\n",
      "з.\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d0477be61f147a59bee7a8162e9e559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing words:   0%|          | 0/1117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 log likelihood: -26081.16796875\n",
      "        negative log likelihood: 26081.16796875\n",
      "average negative log likelihood: 2.4912760257720947\n"
     ]
    }
   ],
   "source": [
    "counter.make_more_prob(10)\n",
    "print('=' * 50)\n",
    "counter.make_more(10)\n",
    "print('=' * 50)\n",
    "counter.loss(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a447782c-ee78-45e8-8816-4c61ef481e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "#\n",
    "#     Now let's get the same results using a Neural Network\n",
    "#\n",
    "# ================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "04fb8bc9-b6fe-4bec-b83f-e148cf870996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self, words, lr=50):\n",
    "        self.chars = ['.'] + sorted(list(set(''.join(words)))) # '.' is going to be used to indicate start and end of words\n",
    "\n",
    "        self.stoi = {s: i for i, s in enumerate(self.chars)}\n",
    "        self.itos = {i: s for s, i in self.stoi.items()}\n",
    "\n",
    "        print(f'total words {len(words)}, total chars {len(self.chars)}, chars{self.chars}')\n",
    "        \n",
    "        # Starting with the code from above iterating over all bigrams,\n",
    "        #   but instead of counting let's form a training set of bigrams (x, y)\n",
    "        #   using 'xs' as inputs and 'ys' as labels\n",
    "\n",
    "        xs, ys = [], []\n",
    "\n",
    "        # for w in words[:1]: # let's first run this for a single word\n",
    "        for w in tqdm(words, 'Processing words'):\n",
    "            word = '.' + w + '.'\n",
    "            for c1, c2 in zip(word, word[1:]):\n",
    "                x = self.stoi[c1]\n",
    "                y = self.stoi[c2]\n",
    "                xs.append(x)\n",
    "                ys.append(y)\n",
    "\n",
    "        self.xs = torch.tensor(xs) # it is important that we should have integers here and below\n",
    "        self.ys = torch.tensor(ys)\n",
    "\n",
    "        print(f'Total bigrams processed {len(self.xs)}')\n",
    "\n",
    "        self.g_make_more = torch.Generator().manual_seed(2147483647)\n",
    "        self.g_weights = torch.Generator().manual_seed(2147483647)\n",
    "        # self.g_weights = torch.Generator()\n",
    "        \n",
    "        # Now we can create a random weight matrix and then multiply by input vector (see x_enc below)\n",
    "        # In order to have a (1, 27) probability distribution vector for each x the weight matrix should be (27, 27)\n",
    "        # Such matrix represents a single linear layer of our NN and\n",
    "        # the forward pass is going to be XW + B (in our case B = 0)\n",
    "        self.W = torch.randn((len(self.chars), len(self.chars)), # 27 neurons NN, each neuron has 27 inputs\n",
    "                             generator=self.g_weights,\n",
    "                             requires_grad=True)\n",
    "\n",
    "        self.loss = None\n",
    "        self.lr = lr\n",
    "        self.P = None\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self):\n",
    "\n",
    "        # for xs (the inputs) instead of just ints (char codes) it is more convenient\n",
    "        # (in terms of vector operations) to use one-hot transformed values instead\n",
    "\n",
    "        x_enc = F.one_hot(self.xs, num_classes=len(self.chars)).float() # the vectors should be float to fit into NN\n",
    "\n",
    "        # print(self.x_enc)\n",
    "        # print(f'{self.x_enc.shape=}')\n",
    "        # plt.imshow(self.x_enc)\n",
    "\n",
    "        # The output is the updated probability matrix (tensor)\n",
    "        # To construct a probability matrix we are going to use exp() to get positive floats as\n",
    "        #   we consider original values to be log(count), so count = exp(log(count)),\n",
    "        #   and then again we normalize the counts to get the probabilities as we did above\n",
    "\n",
    "        logits = x_enc @ self.W # '@' is matrix multiplication in torch; log(count) pridiction we get here\n",
    "\n",
    "        # The next two lines together are called 'Softmax' and turn random positive and negative numbers\n",
    "        #   into a probability distribution vector, all the values of which sum to 1\n",
    "        count = logits.exp() # exp(log(count)) = count\n",
    "        self.P = count / count.sum(1, keepdim=True) # mind keeping the dimentions explicitly as explained above\n",
    "\n",
    "        # Calculating loss manually\n",
    "\n",
    "        # nlls = torch.zeros(5) # 5 = len(self.xs) here\n",
    "        # for i in range(5):\n",
    "        #     xi = self.xs[i].item() # input character\n",
    "        #     yi = self.ys[i].item() # label\n",
    "        #     p = self.P[i, yi]\n",
    "        #     ll = torch.log(p)\n",
    "        #     nll = -ll\n",
    "        #     nlls[i] = nll\n",
    "        #     print(f'{i+1}. Input input {itos[xi]} target {itos[yi]} guess {P[i]}, '+\n",
    "        #           f'probability assigned {p.item()} log likelihood {ll.item()} nll {nll.item()}')\n",
    "        #\n",
    "        # print(f'Average nll (loss) {nlls.mean().item()}')\n",
    "\n",
    "        # Calculating loss\n",
    "        self.loss = -self.P[torch.arange(len(self.xs)), self.ys].log().mean()\n",
    "        return self\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self):\n",
    "        self.W.grad = None # reset all gradients from the previous pass\n",
    "        self.loss.backward()\n",
    "        self.W.data += -self.lr * self.W.grad\n",
    "        return self\n",
    "        \n",
    "    def train(self):\n",
    "        self.forward()\n",
    "        self.backward()\n",
    "        return self\n",
    "    \n",
    "    def learn(self, x=100):\n",
    "        for i in tqdm(range(x), 'Learning progress'):\n",
    "            self.train()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def make_more(self, x=10):\n",
    "        self.forward()\n",
    "        for i in range(x):\n",
    "            word = []\n",
    "            ix = 0 # stoi['.']\n",
    "            while True:\n",
    "                x_enc = F.one_hot(torch.tensor([ix]), num_classes=len(self.chars)).float()\n",
    "                logits = x_enc @ self.W\n",
    "                count = logits.exp()\n",
    "                p = count / count.sum(1, keepdim=True)\n",
    "                ix = torch.multinomial(p, num_samples=1, replacement=True, generator=self.g_make_more).item()\n",
    "                word.append(self.itos[ix])\n",
    "                if ix == 0:\n",
    "                    break\n",
    "\n",
    "            print(''.join(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "873ae24e-323a-4800-8047-6285e9df1724",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words 251337, total chars 35, chars['.', '-', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94953ec72fa843f98c40bb00bd8ee815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing words:   0%|          | 0/251337 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bigrams processed 2330339\n"
     ]
    }
   ],
   "source": [
    "nn = NN(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6e3cdec-e161-4f7d-be7e-dbcceb1b79cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae18305575442b38d379f06d3f722b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Learning progress:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.loss=tensor(2.4189, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "nn.learn(100)\n",
    "print(f'{nn.loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1462560-737c-4ad2-8878-f04b1283e9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "к.\n",
      "мибубаниноле.\n",
      "новоляноволо.\n",
      "бежейск.\n",
      "амэродов.\n",
      "тай лыбав.\n",
      "к.\n",
      "пелещарб-атник.\n",
      "жнь.\n",
      "юразновогускивагурстовецговск.\n",
      "з.\n",
      "нглюрск.\n",
      "овоско.\n",
      "амав.\n",
      "цк.\n",
      "няньспызовецый.\n",
      "скотрк.\n",
      "цгул.\n",
      "ся нрлеранаернстеегбимрганснсана.\n",
      "ов.\n"
     ]
    }
   ],
   "source": [
    "nn.make_more(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dff2cc11-2e74-49b5-9020-9b7c52cb9bc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nn.P[torch.arange(len(nn.xs)), nn.ys].shape=torch.Size([10469])\n",
      "nn.P[:,nn.ys].shape=torch.Size([10469, 10469])\n"
     ]
    }
   ],
   "source": [
    "print(f'{nn.P[torch.arange(len(nn.xs)), nn.ys].shape=}')\n",
    "print(f'{nn.P[:,nn.ys].shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c4d89-9a6d-4540-82d2-d85fed5bcc09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
